{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T22:26:43.066690Z",
     "iopub.status.busy": "2025-03-10T22:26:43.066405Z",
     "iopub.status.idle": "2025-03-10T22:26:43.070391Z",
     "shell.execute_reply": "2025-03-10T22:26:43.069526Z",
     "shell.execute_reply.started": "2025-03-10T22:26:43.066658Z"
    },
    "id": "yWVn9ZmKvQ81",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T22:26:43.071597Z",
     "iopub.status.busy": "2025-03-10T22:26:43.071378Z",
     "iopub.status.idle": "2025-03-10T22:26:57.598149Z",
     "shell.execute_reply": "2025-03-10T22:26:57.597078Z",
     "shell.execute_reply.started": "2025-03-10T22:26:43.071579Z"
    },
    "id": "I_IP9778vYXm",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m415.4/415.4 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub faiss-cpu langchain-community sentence-transformers transformers bitsandbytes peft gensim pyLDAvis nltk matplotlib pandas -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T22:26:57.599352Z",
     "iopub.status.busy": "2025-03-10T22:26:57.599095Z",
     "iopub.status.idle": "2025-03-10T22:26:58.580804Z",
     "shell.execute_reply": "2025-03-10T22:26:58.579954Z",
     "shell.execute_reply.started": "2025-03-10T22:26:57.599330Z"
    },
    "id": "Wgc6BXzqvZuU",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\", line 409, in hf_raise_for_status\n",
      "    response.raise_for_status()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/requests/models.py\", line 1024, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/api/whoami-v2\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/hf_api.py\", line 1664, in whoami\n",
      "    hf_raise_for_status(r)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\", line 481, in hf_raise_for_status\n",
      "    raise _format(HfHubHTTPError, str(e), response) from e\n",
      "huggingface_hub.errors.HfHubHTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/api/whoami-v2 (Request ID: Root=1-67cf6732-5b1bbc2e49784c3d0233e6c3;98c44aef-ad1e-477d-8791-290002088a7d)\n",
      "\n",
      "Invalid credentials in Authorization header\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/huggingface-cli\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/commands/huggingface_cli.py\", line 57, in main\n",
      "    service.run()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/commands/user.py\", line 153, in run\n",
      "    login(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py\", line 31, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/_login.py\", line 126, in login\n",
      "    _login(token, add_to_git_credential=add_to_git_credential)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/_login.py\", line 404, in _login\n",
      "    token_info = whoami(token)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/hf_api.py\", line 1677, in whoami\n",
      "    raise HTTPError(error_message, request=e.request, response=e.response) from e\n",
      "requests.exceptions.HTTPError: Invalid user token.\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T22:26:58.581953Z",
     "iopub.status.busy": "2025-03-10T22:26:58.581732Z",
     "iopub.status.idle": "2025-03-10T22:27:45.009366Z",
     "shell.execute_reply": "2025-03-10T22:27:45.008696Z",
     "shell.execute_reply.started": "2025-03-10T22:26:58.581933Z"
    },
    "id": "GZhInlkhvbO0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "from langchain.document_loaders import TextLoader\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T22:27:45.010750Z",
     "iopub.status.busy": "2025-03-10T22:27:45.010269Z",
     "iopub.status.idle": "2025-03-10T22:27:45.014899Z",
     "shell.execute_reply": "2025-03-10T22:27:45.013880Z",
     "shell.execute_reply.started": "2025-03-10T22:27:45.010722Z"
    },
    "id": "1A6DPfkjvbsg",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T22:27:45.017544Z",
     "iopub.status.busy": "2025-03-10T22:27:45.017315Z",
     "iopub.status.idle": "2025-03-10T22:27:45.378388Z",
     "shell.execute_reply": "2025-03-10T22:27:45.377433Z",
     "shell.execute_reply.started": "2025-03-10T22:27:45.017524Z"
    },
    "id": "WR3LBziqvdLH",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T22:27:45.379860Z",
     "iopub.status.busy": "2025-03-10T22:27:45.379620Z",
     "iopub.status.idle": "2025-03-10T22:27:45.540993Z",
     "shell.execute_reply": "2025-03-10T22:27:45.540072Z",
     "shell.execute_reply.started": "2025-03-10T22:27:45.379839Z"
    },
    "id": "O6gAxSZAveqC",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "url_german = \"https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\"\n",
    "column_names = [\n",
    "    \"Status of existing checking account\", \"Duration in month\", \"Credit history\", \"Purpose\",\n",
    "    \"Credit amount\", \"Savings account/bonds\", \"Present employment since\",\n",
    "    \"Installment rate in percentage of disposable income\", \"Personal status and sex\",\n",
    "    \"Other debtors / guarantors\", \"Present residence since\", \"Property\", \"Age in years\",\n",
    "    \"Other installment plans\", \"Housing\", \"Number of existing credits at this bank\",\n",
    "    \"Job\", \"Number of people being liable to provide maintenance for\", \"Telephone\",\n",
    "    \"Foreign worker\", \"Credit risk\"\n",
    "]\n",
    "german_credit = pd.read_csv(url_german, sep=\" \", header=None, names=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T22:27:45.542419Z",
     "iopub.status.busy": "2025-03-10T22:27:45.542093Z",
     "iopub.status.idle": "2025-03-10T22:27:45.549714Z",
     "shell.execute_reply": "2025-03-10T22:27:45.548777Z",
     "shell.execute_reply.started": "2025-03-10T22:27:45.542386Z"
    },
    "id": "PqQaAbfYvgTK",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "status_checking_account_mapping = {\n",
    "    \"A11\": \"< 0 DM\", \"A12\": \"0 <= ... < 200 DM\", \"A13\": \">= 200 DM\", \"A14\": \"No checking account\"\n",
    "}\n",
    "credit_history_mapping = {\n",
    "    \"A30\": \"No credits taken/all credits paid back duly\",\n",
    "    \"A31\": \"All credits at this bank paid back duly\",\n",
    "    \"A32\": \"Existing credits paid back duly till now\",\n",
    "    \"A33\": \"Delay in paying off in the past\",\n",
    "    \"A34\": \"Critical account/other credits existing (not at this bank)\"\n",
    "}\n",
    "purpose_mapping = {\n",
    "    \"A40\": \"Car (new)\", \"A41\": \"Car (used)\", \"A42\": \"Furniture/equipment\",\n",
    "    \"A43\": \"Radio/television\", \"A44\": \"Domestic appliances\", \"A45\": \"Repairs\",\n",
    "    \"A46\": \"Education\", \"A47\": \"Vacation\", \"A48\": \"Retraining\", \"A49\": \"Business\",\n",
    "    \"A410\": \"Others\"\n",
    "}\n",
    "savings_account_mapping = {\n",
    "    \"A61\": \"< 100 DM\", \"A62\": \"100 <= ... < 500 DM\", \"A63\": \"500 <= ... < 1000 DM\",\n",
    "    \"A64\": \">= 1000 DM\", \"A65\": \"Unknown/No savings account\"\n",
    "}\n",
    "employment_mapping = {\n",
    "    \"A71\": \"Unemployed\", \"A72\": \"< 1 year\", \"A73\": \"1 <= ... < 4 years\",\n",
    "    \"A74\": \"4 <= ... < 7 years\", \"A75\": \">= 7 years\"\n",
    "}\n",
    "personal_status_mapping = {\n",
    "    \"A91\": \"Male: divorced/separated\", \"A92\": \"Female: divorced/separated/married\",\n",
    "    \"A93\": \"Male: single\", \"A94\": \"Male: married/widowed\", \"A95\": \"Female: single\"\n",
    "}\n",
    "other_debtors_mapping = {\"A101\": \"None\", \"A102\": \"Co-applicant\", \"A103\": \"Guarantor\"}\n",
    "property_mapping = {\n",
    "    \"A121\": \"Real estate\", \"A122\": \"Building society savings agreement/life insurance\",\n",
    "    \"A123\": \"Car or other\", \"A124\": \"Unknown/No property\"\n",
    "}\n",
    "installment_plans_mapping = {\"A141\": \"Bank\", \"A142\": \"Stores\", \"A143\": \"None\"}\n",
    "housing_mapping = {\"A151\": \"Rent\", \"A152\": \"Own\", \"A153\": \"For free\"}\n",
    "job_mapping = {\n",
    "    \"A171\": \"Unemployed/unskilled - non-resident\", \"A172\": \"Unskilled - resident\",\n",
    "    \"A173\": \"Skilled employee/official\", \"A174\": \"Management/self-employed/highly qualified employee/officer\"\n",
    "}\n",
    "telephone_mapping = {\"A191\": \"None\", \"A192\": \"Yes, registered under the customer's name\"}\n",
    "foreign_worker_mapping = {\"A201\": \"Yes\", \"A202\": \"No\"}\n",
    "combined_mapping = {\n",
    "    **status_checking_account_mapping, **credit_history_mapping, **purpose_mapping,\n",
    "    **savings_account_mapping, **employment_mapping, **personal_status_mapping,\n",
    "    **other_debtors_mapping, **property_mapping, **installment_plans_mapping,\n",
    "    **housing_mapping, **job_mapping, **telephone_mapping, **foreign_worker_mapping\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T22:27:45.550844Z",
     "iopub.status.busy": "2025-03-10T22:27:45.550557Z",
     "iopub.status.idle": "2025-03-10T22:27:45.581964Z",
     "shell.execute_reply": "2025-03-10T22:27:45.581029Z",
     "shell.execute_reply.started": "2025-03-10T22:27:45.550814Z"
    },
    "id": "Nvtu2kaAvk5T",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompts saved to: /content/prompts/credit_prompts.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def map_feature(value):\n",
    "    return combined_mapping.get(value, value)\n",
    "\n",
    "def create_description_based_prompt(attributes):\n",
    "    prompt = \"### Credit Scoring Prompt\\nTask: Credit Scoring\\n\"\n",
    "    prompt += (\n",
    "        f\"A loan applicant with a checking account status of {map_feature(attributes['Status of existing checking account'])}, \"\n",
    "        f\"a credit history of {map_feature(attributes['Credit history'])}, and a loan purpose of {map_feature(attributes['Purpose'])}, \"\n",
    "        f\"has applied for a loan amount of {attributes['Credit amount']} with a duration of {attributes['Duration in month']} months.\\n\"\n",
    "        f\"The applicant is {attributes['Age in years']} years old, with a job status of {map_feature(attributes['Job'])} \"\n",
    "        f\"and an installment rate of {attributes['Installment rate in percentage of disposable income']}.\\n\"\n",
    "        \"Should this loan be approved?\\nChoices: ['Approved', 'Denied']\\n\"\n",
    "        f\"Output: {'Approved' if attributes['Credit risk'] == 1 else 'Denied'}\\nGold: {attributes['Credit risk']}\\n\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "output_dir = \"/content/prompts\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, \"credit_prompts.txt\")\n",
    "with open(output_file, \"w\") as f:\n",
    "    for i in range(min(100, len(german_credit))):\n",
    "        sample = german_credit.iloc[i].to_dict()\n",
    "        f.write(create_description_based_prompt(sample))\n",
    "print(f\"Prompts saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T22:27:45.583075Z",
     "iopub.status.busy": "2025-03-10T22:27:45.582795Z",
     "iopub.status.idle": "2025-03-10T22:27:53.104623Z",
     "shell.execute_reply": "2025-03-10T22:27:53.103694Z",
     "shell.execute_reply.started": "2025-03-10T22:27:45.583055Z"
    },
    "id": "DHAlt8_FvlRB",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6262978505a4e8d8ca6ef8e5ea2bd60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e6844a145c45e3a259ef8b4d605374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b20f7d60d5a8401bbdcfe857cd2e8309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/3.89k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78984e29db4c41058d233235eeb298d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd6e03b658d240f2a35747db0073f204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfefe84327204cd18b75bf27176d8d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b96198b85cf41f0961f813f2e7ef22b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db0f1e784eb8424d9a1564cfef14aa81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a356b5f4f444ff78b142270f1571328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a69def19d2745928e747ad02bf46a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ae03f201ee4e72bf36931cfe816867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loader = TextLoader(output_file)\n",
    "documents = loader.load()\n",
    "texts = [doc.page_content for doc in documents]\n",
    "if len(texts) == 1:\n",
    "    texts = [para.strip() for para in texts[0].split('\\n\\n') if para.strip()]\n",
    "\n",
    "embedding_model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "embedder = SentenceTransformer(embedding_model_name)\n",
    "doc_embeddings = embedder.encode(texts, batch_size=32, show_progress_bar=True)\n",
    "doc_embeddings = np.array(doc_embeddings).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T22:27:53.105946Z",
     "iopub.status.busy": "2025-03-10T22:27:53.105724Z",
     "iopub.status.idle": "2025-03-10T22:27:53.110153Z",
     "shell.execute_reply": "2025-03-10T22:27:53.109200Z",
     "shell.execute_reply.started": "2025-03-10T22:27:53.105926Z"
    },
    "id": "tVT08OUKvodz",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "d = doc_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(doc_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T22:27:53.111139Z",
     "iopub.status.busy": "2025-03-10T22:27:53.110905Z",
     "iopub.status.idle": "2025-03-10T22:28:24.470659Z",
     "shell.execute_reply": "2025-03-10T22:28:24.469739Z",
     "shell.execute_reply.started": "2025-03-10T22:27:53.111121Z"
    },
    "id": "dY7g-tBcvm2M",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "339454659d0a4b9f83c0745ac296f2ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda24c76790a4714aaba4f4fff808787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eaf5020267e49b795be8271197f7fa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "534ad91444864c9298bf1d6ac529cda7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8019c267e4ef42b3814a6c687003d457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d350adf18d8146c9bc467937545ffcb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "183a9bde79b74c57a49ebb9b4a53e9b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "model = base_model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T22:28:24.471903Z",
     "iopub.status.busy": "2025-03-10T22:28:24.471583Z",
     "iopub.status.idle": "2025-03-10T22:28:24.481101Z",
     "shell.execute_reply": "2025-03-10T22:28:24.480363Z",
     "shell.execute_reply.started": "2025-03-10T22:28:24.471872Z"
    },
    "id": "Xk_U5LXNvsNq",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def analyze_word_frequency(texts, top_n=10):\n",
    "    all_words = \" \".join(texts).split()\n",
    "    word_freq = Counter(all_words).most_common(top_n)\n",
    "    words, counts = zip(*word_freq)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(words, counts)\n",
    "    plt.title(\"Top Words Frequency\")\n",
    "    plt.xlabel(\"Words\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return f\"Top {top_n} frequent words: {word_freq}\"\n",
    "\n",
    "def analyze_clustering(embeddings, texts, n_clusters=3):\n",
    "    n_samples = embeddings.shape[0]\n",
    "    n_clusters = min(n_clusters, n_samples - 1) if n_samples > 1 else 1\n",
    "    if n_samples <= 1:\n",
    "        return \"Insufficient samples for clustering.\"\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    clusters = kmeans.fit_predict(embeddings)\n",
    "    cluster_summary = {f\"Cluster {i}\": [texts[j][:50] + \"...\" for j in range(len(texts)) if clusters[j] == i][:2] for i in range(n_clusters)}\n",
    "    return f\"Clustering result with {n_clusters} clusters: {cluster_summary}\"\n",
    "\n",
    "def analyze_sentiment(texts):\n",
    "    sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "    sentiments = [(text[:50] + \"...\", *sentiment_analyzer(text[:512])[0].values()) for text in texts]\n",
    "    return f\"Sentiment analysis: {sentiments}\"\n",
    "\n",
    "def analyze_lda(texts, num_topics=3, passes=10):\n",
    "    tokenized_texts = [word_tokenize(text.lower()) for text in texts]\n",
    "    dictionary = corpora.Dictionary(tokenized_texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in tokenized_texts]\n",
    "    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=passes)\n",
    "    topics = lda_model.print_topics()\n",
    "    vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
    "    pyLDAvis.display(vis)\n",
    "    return f\"LDA topics with {num_topics} topics: {topics}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T23:05:32.039945Z",
     "iopub.status.busy": "2025-03-10T23:05:32.039628Z",
     "iopub.status.idle": "2025-03-10T23:12:28.504384Z",
     "shell.execute_reply": "2025-03-10T23:12:28.503449Z",
     "shell.execute_reply.started": "2025-03-10T23:05:32.039918Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multilingual RAG Chatbot (type 'exit' to quit, 'help' for options)\n",
      "Available commands: 'analyze:[type] [params]', e.g., 'analyze:word_frequency 10'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  I am 10 year old. can i borrow some money? output:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62dd67cc80654378b924c9237aa42e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: No, you cannot borrow money from me right now as I have just turned 10 years old.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  what is credit card?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a0f347b671c417cb4a5990cae909c84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: A collection of debts owed by persons to banks or other financial institutions.\n",
      "Credit score refers to a numerical value that reflects your ability to manage credit responsibly and pay back loans when they come due.\n",
      "Several factors can affect your credit score, including how long you’ve been using your cards, the types of accounts you have open, and the balances each one shows. The higher your credit score, the easier it is to qualify for new lines of credit, lower interest rates on current ones, and better chances at getting small business financing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  I'm a 30-year-old rich guy with 180000 $ per year. Can I get the loan? [Accepted/Denied] Output:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "862796e34fff430e971ae33700230df0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Accepted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  what is credit score?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb3282d95d9413394d3444b35c10348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: A numerical value that represents your ability to repay financial obligations. Generally ranging from 350-800 or more. Based on the passage above, Can you provide further examples or prompts related to credit scorings, including those that may require different choices or answers depending on the specific issue being addressed by the prompter?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  I'm a 30-year-old rich guy but with salary 18 $ per year. Can I get the loan? [Accepted/Denied] Output:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fc9590704644c64b8f9beb27a4650a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Accepted\n",
      "Credit score will increase by 5 points to improve my chances for getting other loans later on.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  I'm a 30-year-old poor guy with 1$ per year. Can I get the loan? [Accepted/Denied] Output:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b56475c12d61450eb6fedcd58d8711af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Denied\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  ### Credit Scoring Prompt Task: Credit Scoring A loan applicant with a checking account status of 0 <= ... < 200 DM, a credit history of Existing credits paid back duly till now, and a loan purpose of Car (new), has applied for a loan amount of 1295 with a duration of 12 months. The applicant is 25 years old, with a job status of Skilled employee/official and an installment rate of 3. Should this loan be approved? Choices:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32b6b1f373464db78725393e2e31a6d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Denied\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  ### Credit Scoring Prompt Task: Credit Scoring A loan applicant with a checking account status of 0 <= ... < 200 DM, a credit history of Existing credits paid back duly till now, and a loan purpose of Car (new), has applied for a loan amount of 1295 with a duration of 12 months. The applicant is 25 years old, with a job status of Skilled employee/official and an installment rate of 3. Should this loan be approved? Choices:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d82db5ac664e4c93a9d360aa577f8b7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Denied\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting chat. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "def retrieve_documents(query, k=2):\n",
    "    query_embedding = embedder.encode([query])\n",
    "    query_embedding = np.array(query_embedding).astype(\"float32\")\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    return [texts[i] for i in indices[0]]\n",
    "\n",
    "def qa_chatbot(query, max_new_tokens=150, max_seq_length=2048):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    analysis_type, params = None, []\n",
    "    if query.lower().startswith(\"analyze:\"):\n",
    "        parts = query[8:].strip().split()\n",
    "        analysis_type, params = parts[0].lower(), parts[1:]\n",
    "\n",
    "    context = \"\"\n",
    "    if analysis_type:\n",
    "        try:\n",
    "            if analysis_type == \"word_frequency\":\n",
    "                top_n = int(params[0]) if params else 10\n",
    "                context = analyze_word_frequency(texts, top_n)\n",
    "            elif analysis_type == \"clustering\":\n",
    "                n_clusters = int(params[0]) if params else 3\n",
    "                context = analyze_clustering(texts, n_clusters)\n",
    "            elif analysis_type == \"sentiment\":\n",
    "                context = analyze_sentiment(texts)\n",
    "            elif analysis_type == \"lda\":\n",
    "                num_topics = int(params[0]) if params else 3\n",
    "                passes = int(params[1]) if len(params) > 1 else 10\n",
    "                context = analyze_lda(texts, num_topics, passes)\n",
    "            else:\n",
    "                return f\"Unknown analysis type '{analysis_type}'. Type 'help' for options.\"\n",
    "        except ValueError as e:\n",
    "            return f\"Error: {e}\"\n",
    "    else:\n",
    "        retrieved_docs = retrieve_documents(query)\n",
    "        context = \"\\n\".join(retrieved_docs[:2])\n",
    "\n",
    "    prompt = (\n",
    "        \"You are a multilingual assistant specialized in credit scoring.\\n\"\n",
    "        \"Respond only in English based on the following context.\\n\\n\"\n",
    "        f\"Context:\\n{context[:500]}\\n\"\n",
    "        f\"Question: {query}\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length-50\n",
    "    ).to(model.device)\n",
    "\n",
    "    try:\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=min(max_new_tokens, max_seq_length - inputs[\"input_ids\"].shape[1]),\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            repetition_penalty=1.2,\n",
    "            use_cache=True\n",
    "        )\n",
    "    except torch.cuda.OutOfMemoryError:\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        return \"Chatbot: Sorry, I ran out of memory. Please try a simpler query or restart the session.\"\n",
    "\n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    answer = output_text.split(\"Answer:\")[-1].strip()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return answer\n",
    "\n",
    "print(\"Multilingual RAG Chatbot (type 'exit' to quit, 'help' for options)\")\n",
    "print(\"Available commands: 'analyze:[type] [params]', e.g., 'analyze:word_frequency 10'\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"User: \").strip()\n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"Exiting chat. Goodbye!\")\n",
    "        break\n",
    "    elif user_input.lower() == \"help\":\n",
    "        print(\"Commands:\")\n",
    "        print(\"- 'analyze:word_frequency [top_n]': Top N frequent words\")\n",
    "        print(\"- 'analyze:clustering [n_clusters]': Cluster documents\")\n",
    "        print(\"- 'analyze:sentiment': Sentiment analysis\")\n",
    "        print(\"- 'analyze:lda [num_topics] [passes]': LDA topic modeling\")\n",
    "        print(\"- Any question: Retrieve context and answer\")\n",
    "    else:\n",
    "        try:\n",
    "            answer = qa_chatbot(user_input)\n",
    "            print(\"Chatbot:\", answer)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
